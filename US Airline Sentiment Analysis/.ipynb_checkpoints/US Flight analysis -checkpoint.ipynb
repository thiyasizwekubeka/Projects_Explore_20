{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The airline industry is a very competitive market which has grown rapidly in the past 2 decades. Airline companies resort to traditional customer feedback forms which in turn are very tedious and time consuming. This is where Twitter data serves as a good source to gather customer feedback tweets and perform a sentiment analysis\n",
    "\n",
    "### Problem statement\n",
    "\n",
    "Here i will be looking at US Airlines twitter data to  predict the accuracy of how customer tweet relates to a class sentiment(negative, positive, neutral). This is a classic case of a sentiment analysis in machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Style\n",
    "sns.set(font_scale=1.5)\n",
    "plt.style.use('seaborn-pastel')\n",
    "plt.style.use('seaborn-poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0d174e622d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweets.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting tweets for each sentiment class\n",
    "\n",
    "From the results below we can see that the most shared sentiments based on US Airlines are negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df['airline_sentiment'].value_counts()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall sentiments for US airline\n",
    "\n",
    "Graphically we can also confirm that the most shared sentiments for US Airlines are negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index = [1,2,3]\n",
    "plt.bar(Index, df1)\n",
    "plt.xticks(Index, ['negative','neutral','positive'])\n",
    "plt.ylabel('sentiment count')\n",
    "plt.xlabel('sentiment class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can see from the above graph the sentiments which are most shared about US airlines are largely negative. This could be due to a factor of reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Sentiments shared for each airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.airline, df.airline_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pie chart of tweets frequency for each airline\n",
    "labels = ['United','US Airways','American','Southwest','Delta','Virgin America']\n",
    "sizes = [0.261, 0.198, 0.188, 0.165, 0.152, 0.0344]\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#33ccff', '#ff6600']\n",
    "fig1, ax1 = plt.subplots(figsize=(6.5, 6))\n",
    "ax1.pie(sizes, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "ax1.axis('equal')  \n",
    "plt.tight_layout()\n",
    "plt.title('Tweets Frequency by Airline', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation \n",
    "United Airlines has the most negative sentiments but if you look at how frequently people tweet about the airline , it becomes evident that a possible reason why the United airline has so much negative feedback could be because it is the most frequently used airline in the United States(possibly due to affordability). We can further assume that because the airline is so affordable, not a lot services are offered ,as it will later be seen from a visual representing the reason for the negative sentiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason for people sharing negative sentiments for each airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df.groupby(\"negativereason\")['airline'].value_counts(normalize=False).sort_index()\n",
    "types.unstack().plot(kind='barh', stacked='True')\n",
    "plt.legend(bbox_to_anchor=(1.5, 1), loc='upper right')\n",
    "plt.xlabel('Number of tweets')\n",
    "plt.title('Distribution of Number of negative tweets for every Airline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "As previously seen the United Airline shares the most negative sentiments. One conclusion that i made was that it an affordable airline and due to the fact , it doesn't offer a lot of customer friendly services. If we look at the above graph we can see that United airlines accounts for a lot of the negative reasons shared concerning an airline and their biggest issue big that of customer service issues. This agrees with my initial generalisation , which suggested that the flight is the most frequently used with a possible reason being its good affordability and becuase of that fact , not a lot of customer friendly services are offered. These customer Service issues include waiting for a long period of time on the phone to speak to a consulted, not getting a response in time if you send an email about an issue, the customer support allocating tickets to the wrong department.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 negative reasons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.negativereason.value_counts().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block , i wrote a script to clean the data. This will make it easier for my models to generalize the data better or improve the performance of the model. \n",
    "\n",
    "Looking at my dataset what needs to be cleaned is the text feature(which is basically the comments relating to the sentiment shared by the customers on twitter), as it has a lot of stop words and unnecessary punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = '@[A-Za-z0-9]+'\n",
    "url = 'https?://[^ ]+'\n",
    "link = 'www.[^ ]+'\n",
    "combined_p = '|'.join((username, url, link))\n",
    "neg_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg = re.compile('\\b(' + '|'.join(neg_dic.keys()) + ')\\b')\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    stripped = re.sub(combined_p, '', text)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg.sub(lambda x: neg_dic[x.group()], lower_case)\n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = [x for x in tok.tokenize(letters) if len(x) > 1]\n",
    "    drop_stopwords = [x for x in words if x not in stopwords.words('english')]\n",
    "    return (\" \".join(drop_stopwords)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = []\n",
    "for tweet in df.text:\n",
    "    clean_tweets.append(tweet_cleaner(tweet))\n",
    "df['clean_text'] = pd.DataFrame(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['text'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the amount of missing values\n",
    "\n",
    "\n",
    "A data set with a lot of missing values makes it hard for the model to create a general observation about the data. So the way i dealt with this issue , was by finding out the percentage of the missing values in each column and drop the columns with the highest missing values because they are considered redunant columns. For columns with few missing values , i used Imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'percent_missing': percent_missing})\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['airline_sentiment_gold','negativereason_gold','tweet_coord'],axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['negativereason_confidence'].fillna(df['negativereason_confidence'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=nan, strategy='most_frequent')\n",
    "df.negativereason = imputer.fit_transform(df['negativereason'].values.reshape(-1,1))[:,0]\n",
    "df.tweet_location = imputer.fit_transform(df['tweet_location'].values.reshape(-1,1))[:,0]\n",
    "df.user_timezone  = imputer.fit_transform(df['user_timezone'].values.reshape(-1,1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'percent_missing': percent_missing})\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# creating initial dataframe\n",
    "# bridge_types = ('Arch','Beam','Truss','Cantilever','Tied Arch','Suspension','Cable')\n",
    "# bridge_df = pd.DataFrame(bridge_types, columns=['Bridge_Types'])\n",
    "# creating instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "df2['airline_sentiment_en'] = labelencoder.fit_transform(df2['airline_sentiment'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - validation split \n",
    "\n",
    "neutral = 1\n",
    "\n",
    "\n",
    "positive = 2\n",
    "\n",
    "\n",
    "negative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate features and tagret variables\n",
    "y = df2['airline_sentiment_en']\n",
    "X = df2['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, stop_words=\"english\")\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train data to create validation dataset\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=.1,shuffle=True, stratify=y, random_state=11)#changed test size to 0.1 from 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict how customers feel about US airlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "\n",
    "Supervised machine learning algorithm that is generally used for classification problems. It operates by constructing multiple decision trees during the training phase. The random forest chooses the decision of the majority of the trees as the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_roc, plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import time\n",
    "modelstart = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 100 , max_features= 'auto', bootstrap = 'False')\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "rf_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
    "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: shows how many positive classes we predicted correctly \n",
    "        recall should be as high as possible \n",
    "        for neutral sentiments is where we got the highest recall which is a good measure for our class prediction\n",
    "        lowest was negative sentiment \n",
    "        \n",
    "Precision: from all the classes we have predicted as positive , how many are actually positive \n",
    "           Should be as high as possible \n",
    "F1-Score: Helps to measure Recall and Precision at the same time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_val, y_pred, normalize=True,figsize=(8,8),cmap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "A decision tree builds classification or regression models as a tree structure, with datasets broken up into small subsets while developing the decision tree, with branches and nodes. Decision trees can handle both categorical and numerical data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "modelstart = time.time()\n",
    "rf = DecisionTreeClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "dt_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
    "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_val, y_pred, normalize=True,figsize=(8,8),cmap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost \n",
    "\n",
    "It is a supervised learning algorithm where strong predictors is build in additive or sequential manner using weak predictor typically Decision tree. It is used for both classification and regression task.\n",
    "\n",
    "It is ensemble learning algorithm. It is based on strong theoretical concept of sequentially combining weak predictor to build strong predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "modelstart = time.time()\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_val)\n",
    "gb_model_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
    "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_val, y_pred, normalize=True,figsize=(8,8),cmap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC \n",
    "\n",
    "The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "modelstart = time.time() \n",
    "linsvc = LinearSVC()\n",
    "linsvc.fit(X_train, y_train)\n",
    "y_pred = linsvc.predict(X_val)\n",
    "linsvc_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
    "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "results = pd.DataFrame(report).transpose()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_val, y_pred, normalize=True,figsize=(8,8),cmap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation \n",
    "\n",
    "F1 - score metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Weighted F1-Scores Between Models\n",
    "fig,axis = plt.subplots(figsize=(10, 5))\n",
    "rmse_x = ['Random Forest Classifier','Linear SVC','DecisionTreeClassifier','Gradient Boosting Classifier']\n",
    "rmse_y = [rf_f1,dt_f1,linsvc_f1,gb_model_f1]\n",
    "ax = sns.barplot(x=rmse_x, y=rmse_y,palette=(\"Blues_d\"))\n",
    "plt.title('Weighted F1-Score Per Classification Model',fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Weighted F1-Score')\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),2), fontsize=12, ha=\"center\", va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators = number of trees in the foreset\n",
    "\n",
    "\n",
    "max_features = max number of features considered for splitting a node\n",
    "\n",
    "\n",
    "max_depth = max number of levels in each decision tree\n",
    "\n",
    "\n",
    "min_samples_split = min number of data points placed in a node before the node is split\n",
    "\n",
    "\n",
    "min_samples_leaf = min number of data points allowed in a leaf node\n",
    "\n",
    "\n",
    "bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the documentation on the random forest i Scikit-Learn. This tells us the most important settings are the number of trees in the forest(n_estimators) and the number of features considered for splitting at each leaf node(max_features). So we going to try a wide range of values and see what works best then try adjusting the set of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV vs GridSearchCV \n",
    "\n",
    "RandomizedSearchCV: Only few samples in the data are randomly selected\n",
    "\n",
    "\n",
    "GridSearchCv: Considers all possible combinations of hyperparameters\n",
    "\n",
    "## K-fold Cross-validation\n",
    "\n",
    "In K-fold cross-validation the training dataset is divided into three parts as training data , cross validation data and testing data. This is a way to utilise the training data we have as much as possible.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Run RandomizedSearchCV to tune the hyper-parameter\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rfr=RandomForestRegressor()\n",
    "k_fold_cv = 5 # Stratified 5-fold cross validation\n",
    "params = {\n",
    " 'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
    " 'max_features' : ['auto', 'sqrt'],\n",
    " 'bootstrap' : [True, False]\n",
    " }\n",
    "random = RandomizedSearchCV(rfr, param_distributions=params, cv=k_fold_cv,\n",
    " n_iter = 5, scoring='neg_mean_absolute_error',verbose=2, random_state=42,\n",
    " n_jobs=-1, return_train_score=True)\n",
    "random.fit(X_train, y_train)\n",
    "# print('Best hyper parameter:â€™, random.best_params_)\n",
    "print('Best hyper parameter:', random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearch to tune the hyper-parameter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfr=RandomForestRegressor()\n",
    "k_fold_cv = 5 # Stratified 5-fold cross validation\n",
    "grid_params = {\n",
    " 'n_estimators' : [10, 50,100],\n",
    " 'max_features' : ['auto', 'sqrt'],\n",
    " 'bootstrap' : [True, False]\n",
    " }\n",
    "grid = GridSearchCV(rfr, param_grid = grid_params, cv=k_fold_cv, verbose=0, n_jobs =1, return_train_score=True)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best hyper parameter:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RandomizedSearchCV we observed that our model for the random forest classifier improved by 1,05% , where initally our model accuracy was 72% and after applying the new hyperparameters in moved up to 73%. Though from research we know that a RandomizedSearchCV works works extremely well with a large dataset , the concept of hyperparameter tuning was put to the test and yielded a positive outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
